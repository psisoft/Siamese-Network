{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'x_width': 256,\n",
    "    'x_height': 40,\n",
    "    'num_channels': 1,\n",
    "    'dropout': [0.1, 0.1],\n",
    "    'embedding_size': 256,\n",
    "    'batch_size': 256,\n",
    "    'positives_per_batch_count': 40,\n",
    "    'triplets_count': 1024,\n",
    "    'masked_count': 4,\n",
    "    'alpha_margin': 0.19,\n",
    "    'learning_rate': 2e-5,\n",
    "\n",
    "    'tfrecords_train': './TF_Records_File_train.tfrecords',\n",
    "    'tfrecords_cv': './TF_Records_File_cv.tfrecords',\n",
    "\n",
    "    'model_path': './trained_model.ckpt',\n",
    "    'log_path': './train_log.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator():\n",
    "    # each batch contains an anchor x at index 0 followed by\n",
    "    # config[\"positives_per_batch_count\"] positives followed by\n",
    "    # negatives.\n",
    "    \n",
    "     ### ==> TODO: You need to write this function for your case to use this code\n",
    "    train_ratio = .7\n",
    "    iters = 1e5\n",
    "\n",
    "    for _iter in range(iters):\n",
    "        batch_data = np.zeros([config['batch_size'],\n",
    "                               config['num_channels'],\n",
    "                               config['x_height'],\n",
    "                               config['x_width']], dtype = np.float32)\n",
    "\n",
    "        batch_data[0, :, :, :] = #anchor_data        \n",
    "        batch_data[1:(config['positives_per_batch_count'] + 1), :, :, :] = #positive_data\n",
    "        batch_data[(config['positives_per_batch_count'] + 1):, :, :, :] = #negative_data\n",
    "        \n",
    "        if i <= np.int(train_ratio * iters):\n",
    "            batch_type = 'train'\n",
    "        else:\n",
    "            batch_type = 'cv'\n",
    "        yield np.array(batch_data).tobytes(), batch_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def write_to_tfrecord():\n",
    "    # write batches to tfrecords file\n",
    "    data_generator = batch_generator()\n",
    "    writer_train = tf.python_io.TFRecordWriter(config['tfrecords_train'])\n",
    "    writer_cv = tf.python_io.TFRecordWriter(config['tfrecords_cv'])\n",
    "    train_counter = 0\n",
    "    cv_counter = 0\n",
    "    try:\n",
    "        while(1):\n",
    "            batch, batch_type = next(data_generator)\n",
    "            feature = tf.train.Feature(bytes_list = tf.train.BytesList(value = [batch]))\n",
    "            feature_dict = {'batch':\n",
    "                tf.train.Feature(bytes_list = tf.train.BytesList(value = [batch]))}\n",
    "            example = tf.train.Example(features = tf.train.Features(feature = feature_dict))\n",
    "            if batch_type == 'train':\n",
    "                train_counter += 1\n",
    "                writer_train.write(example.SerializeToString())\n",
    "                print('>>>>>> train: ', train_counter, end = '\\r')\n",
    "            else:\n",
    "                cv_counter += 1\n",
    "                writer_cv.write(example.SerializeToString())\n",
    "                print('>>>>>> cv: ', cv_counter, end = '\\r')\n",
    "    except:\n",
    "        print('')\n",
    "    finally:\n",
    "        print('total_number of trained records:', train_counter)\n",
    "        print('total_number of cv records:', cv_counter)\n",
    "        writer_train.close()\n",
    "        writer_cv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_tfrecord(tfrecord_file, queue_size, num_threads, min_capacity):\n",
    "    # read batch from tfrecords file\n",
    "    with tf.variable_scope('Queue_Batch_Shuffle', reuse = False) as scope:\n",
    "        tfrecord_file_queue = tf.train.string_input_producer(tfrecord_file, name = 'queue')\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, tfrecord_serialized = reader.read(tfrecord_file_queue)\n",
    "        tfrecord_features = tf.parse_single_example(tfrecord_serialized,\n",
    "                    features = {'batch': tf.FixedLenFeature([], tf.string)},\n",
    "                    name = 'features')\n",
    "\n",
    "        batch_data = tf.decode_raw(tfrecord_features['batch'], tf.float32)\n",
    "        batch_data = tf.reshape(batch_data, [config['batch_size'],\n",
    "                                             config['num_channels'],\n",
    "                                             config['x_height'],\n",
    "                                             config['x_width']])\n",
    "\n",
    "        batch_data_shuffled = tf.train.shuffle_batch([batch_data],\n",
    "                                                      batch_size = 1,\n",
    "                                                      capacity = queue_size,\n",
    "                                                      num_threads = num_threads,\n",
    "                                                      min_after_dequeue = min_capacity)\n",
    "        return batch_data_shuffled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_list_maker():\n",
    "    # make a list of <anchor_index, positive_index, negativeIndex> triplets\n",
    "    # of size config['triplets_count']\n",
    "    triplets = []\n",
    "    negatives_per_batch_count = config['batch_size'] - config['positives_per_batch_count']\n",
    "    positives_index = list(range(config['positives_per_batch_count']))\n",
    "    np.random.shuffle(positives_index)\n",
    "    for positive_idx in positives_index:\n",
    "        pos_idx = positive_idx + 1\n",
    "        negatives_index = list(range(negatives_per_batch_count))\n",
    "        np.random.shuffle(negatives_index)\n",
    "        for negative_idx in negatives_index:\n",
    "            neg_idx = negative_idx + 1 + config['positives_per_batch_count']\n",
    "            triplets.append([0, pos_idx, neg_idx])\n",
    "            if len(triplets) == config['triplets_count']:\n",
    "                result = np.array(triplets)\n",
    "                np.random.shuffle(result)\n",
    "                return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x, dropout_rate = [0.1, 0.05], is_training = True, print_summary = False):\n",
    "    # build the CNN model\n",
    "    print('Build model...')\n",
    "    with tf.variable_scope('Model', reuse = False) as scope:\n",
    "        # block 1\n",
    "        conv1 = tf.layers.conv2d(x, 32, [3, 5],\n",
    "                                 strides = [1, 1],\n",
    "                                 data_format = 'channels_first',\n",
    "                                 padding = 'same',\n",
    "                                 name = 'conv1')\n",
    "        batch_norm1 = tf.layers.batch_normalization(conv1, training = is_training,\n",
    "                                                    axis = 1, name = 'batch_norm1')\n",
    "        relu1 = tf.nn.relu(batch_norm1, name = 'relu1')\n",
    "        max_pool1 = tf.layers.max_pooling2d(relu1, [2, 2],\n",
    "                                            strides = [2, 2],\n",
    "                                            data_format = 'channels_first',\n",
    "                                            padding = 'valid',\n",
    "                                            name = 'max_pool1')\n",
    "\n",
    "        # block 2\n",
    "        conv2 = tf.layers.conv2d(max_pool1, 64, [2, 5],\n",
    "                                 strides = [1, 1],\n",
    "                                 data_format = 'channels_first',\n",
    "                                 padding = 'same',\n",
    "                                 name = 'conv2')\n",
    "        batch_norm2 = tf.layers.batch_normalization(conv2, training = is_training,\n",
    "                                                    axis = 1, name = 'batch_norm2')\n",
    "        relu2 = tf.nn.relu(batch_norm2, name = 'relu2')\n",
    "        max_pool2 = tf.layers.max_pooling2d(relu2, [2, 2],\n",
    "                                            strides = [2, 2],\n",
    "                                            data_format = 'channels_first',\n",
    "                                            padding = 'valid',\n",
    "                                            name = 'max_pool2')\n",
    "\n",
    "        # block 3\n",
    "        conv3 = tf.layers.conv2d(max_pool2, 128, [3, 3],\n",
    "                                 strides = [1, 1],\n",
    "                                 data_format = 'channels_first',\n",
    "                                 padding = 'same',\n",
    "                                 name = 'conv3')\n",
    "        batch_norm3 = tf.layers.batch_normalization(conv3, training = is_training,\n",
    "                                                    axis = 1, name = 'batch_norm3')\n",
    "        relu3 = tf.nn.relu(batch_norm3, name = 'relu3')\n",
    "        max_pool3 = tf.layers.max_pooling2d(relu3, [2, 2],\n",
    "                                            strides = [2, 2],\n",
    "                                            data_format = 'channels_first',\n",
    "                                            padding = 'valid',\n",
    "                                            name = 'max_pool3')\n",
    "\n",
    "        # block 4\n",
    "        conv4 = tf.layers.conv2d(max_pool3, 256, [2, 2],\n",
    "                                 strides = [1, 1],\n",
    "                                 data_format = 'channels_first',\n",
    "                                 padding = 'same',\n",
    "                                 name = 'conv4')\n",
    "        batch_norm4 = tf.layers.batch_normalization(conv4, training = is_training,\n",
    "                                                    axis = 1, name = 'batch_norm4')\n",
    "        relu4 = tf.nn.relu(batch_norm4, name = 'relu4')\n",
    "        max_pool4 = tf.layers.max_pooling2d(relu4, [2, 2],\n",
    "                                            strides = [2, 2],\n",
    "                                            data_format = 'channels_first',\n",
    "                                            padding = 'valid',\n",
    "                                            name = 'max_pool4')\n",
    "        dropout4 = tf.layers.dropout(max_pool4, dropout_rate[0], name = 'dropout4')\n",
    "\n",
    "        # block 5\n",
    "        flatten_length = dropout4.get_shape().as_list()[1] * \\\n",
    "                         dropout4.get_shape().as_list()[2] * \\\n",
    "                         dropout4.get_shape().as_list()[3]\n",
    "        flatten5 = tf.reshape(dropout4, (-1, flatten_length), name = 'flatten5')\n",
    "        fc5 = tf.layers.dense(flatten5, 1024, name = 'fc5')\n",
    "        batch_norm5 = tf.layers.batch_normalization(fc5, training = is_training,\n",
    "                                                    name = 'batch_norm5')\n",
    "        relu5 = tf.nn.relu(batch_norm5, name = 'relu5')\n",
    "\n",
    "        # block 6\n",
    "        fc6 = tf.layers.dense(relu5, 1024, name = 'fc6')\n",
    "        batch_norm6 = tf.layers.batch_normalization(fc6, training = is_training,\n",
    "                                                    name = 'batch_norm6')\n",
    "        relu6 = tf.nn.relu(batch_norm6, name = 'relu6')\n",
    "\n",
    "        # block 7\n",
    "        fc7 = tf.layers.dense(relu6, config['embedding_size'], name = 'fc7')\n",
    "        batch_norm7 = tf.layers.batch_normalization(fc7, training = is_training,\n",
    "                                                    name = 'batch_norm7')\n",
    "        relu7 = tf.nn.relu(batch_norm7, name = 'relu7')\n",
    "        dropout7 = tf.layers.dropout(relu7, dropout_rate[1], name = 'dropout7')\n",
    "        l27 = tf.nn.l2_normalize(fc7, 1, name ='l27')\n",
    "\n",
    "        # block 8\n",
    "        fc8 = tf.layers.dense(dropout7, config['embedding_size'], name = 'fc8')\n",
    "        l28 = tf.nn.l2_normalize(fc8, 1, name ='l28')\n",
    "\n",
    "        assert fc8.get_shape()[1] == config['embedding_size']\n",
    "        if print_summary:\n",
    "            print('Model summary:\\n x: %s\\n' \\\n",
    "                  ' conv1: %s\\n max_pool1: %s\\n' \\\n",
    "                  ' conv2: %s\\n max_pool2: %s\\n' \\\n",
    "                  ' conv3: %s\\n max_pool3: %s\\n' \\\n",
    "                  ' conv4: %s\\n max_pool4 %s\\n' \\\n",
    "                  ' flatten5 %s\\n fc5 %s\\n' \\\n",
    "                  ' fc6 %s\\n fc7 %s\\n' \\\n",
    "                  ' fc8 %s\\n' %(x.get_shape(),\n",
    "                                conv1.get_shape(), max_pool1.get_shape(),\n",
    "                                conv2.get_shape(), max_pool2.get_shape(),\n",
    "                                conv3.get_shape(), max_pool3.get_shape(),\n",
    "                                conv4.get_shape(), max_pool4.get_shape(),\n",
    "                                flatten5.get_shape(), fc5.get_shape(),\n",
    "                                fc6.get_shape(), fc7.get_shape(), fc8.get_shape()))\n",
    "        return l27, l28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(loss):\n",
    "    # model optimizer\n",
    "     with tf.variable_scope('Optimizer', reuse = False) as scope:\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            all_vars = tf.trainable_variables()\n",
    "            model_vars = [var for var in all_vars if var.name.startswith('Model')]\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate = config['learning_rate']).minimize(loss, var_list = model_vars)\n",
    "            return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_triplet_embeddings(triplets, embeddings):\n",
    "    # prepare triplet embeddings\n",
    "    # triplets: (anchor_index, positive_index, negative_index)\n",
    "\n",
    "    with tf.variable_scope('Embeddings', reuse = False) as scope:\n",
    "        anchor_embeddings = tf.gather(embeddings, triplets[:, 0])\n",
    "        positive_embeddings = tf.gather(embeddings, triplets[:, 1])        \n",
    "        negative_embeddings = tf.gather(embeddings, triplets[:, 2])\n",
    "\n",
    "        return anchor_embeddings, positive_embeddings, negative_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss_acc(triplet_embeddings1, triplet_embeddings2, mask, threshold):\n",
    "    # triplet loss that is being minimized:\n",
    "    # loss = reduce_mean(l2_norm_squared(f(x_a), f(x_p)) -\n",
    "    #                    l2_norm_squared(f(x_a), f(x_n)) +\n",
    "    #                    alpha)\n",
    "\n",
    "    anchor_embeddings1, positive_embeddings1, nagative_embeddings1 = triplet_embeddings1\n",
    "    anchor_embeddings2, positive_embeddings2, nagative_embeddings2 = triplet_embeddings2\n",
    "\n",
    "    with tf.variable_scope('Loss', reuse = False) as scope:\n",
    "        ap_dist2 = tf.reduce_sum(tf.square(anchor_embeddings2 - positive_embeddings2), axis = -1)\n",
    "        an_dist2 = tf.reduce_sum(tf.square(anchor_embeddings2 - nagative_embeddings2), axis = -1)\n",
    "\n",
    "        flags = tf.cast(tf.greater(an_dist2, ap_dist2), tf.float32)\n",
    "        flags = tf.maximum(flags, mask) # let a few ap > an cases skip and be used in training (for exploration)\n",
    "        base_loss2  = (ap_dist2 - an_dist2 + config['alpha_margin']) * flags\n",
    "        loss2 = tf.reduce_sum(tf.maximum(base_loss2, 0))\n",
    "\n",
    "        ap_dist1 = tf.reduce_sum(tf.square(anchor_embeddings1 - positive_embeddings1), axis = -1)\n",
    "        an_dist1 = tf.reduce_sum(tf.square(anchor_embeddings1 - nagative_embeddings1), axis = -1)\n",
    "        base_loss1  = (ap_dist1 - an_dist1 + config['alpha_margin']) * flags\n",
    "        loss1 = tf.reduce_sum(tf.maximum(base_loss1, 0))\n",
    "\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        ap_acc = tf.reduce_mean(tf.cast(tf.greater(threshold, tf.sqrt(ap_dist2)), tf.float32))\n",
    "        an_acc = tf.reduce_mean(tf.cast(tf.greater(tf.sqrt(an_dist2), threshold), tf.float32))\n",
    "        triplets_used = tf.reduce_sum(flags)\n",
    "\n",
    "        return loss, loss2, ap_acc, an_acc, triplets_used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_count = 1234567 # put your number here\n",
    "cv_batches_count = 12345 # put your number here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_on_imporvemnet(file_path, sess, cv_loss, cv_losses):\n",
    "  #  save model when there is improvemnet in cv_loss value\n",
    "    if cv_losses == [] or cv_loss < np.min(cv_losses):\n",
    "        # save the entire model\n",
    "        saver = tf.train.Saver(max_to_keep = 1)\n",
    "        saver.save(sess, file_path)\n",
    "        print('Model saved')\n",
    "\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(file_path, epoch, train_loss, cv_loss, log_mode = 'a'):\n",
    "    # log train and cv losses\n",
    "    mode = log_mode if epoch == 0 else 'a'\n",
    "\n",
    "    with open(file_path, mode) as f:\n",
    "        if mode == 'w':\n",
    "            header = 'epoch, train_loss, cv_loss\\n'\n",
    "            f.write(header)\n",
    "\n",
    "        line = '%d, %f, %f\\n' %(epoch, train_loss, cv_loss)\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cool_down_mask(epoch):\n",
    "    if epoch > 0 and epoch % 20 == 0:\n",
    "        if config['masked_count'] >= 1:\n",
    "            config['masked_count'] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrease_learning_rate(epoch):\n",
    "    #if epoch > 0 and epoch % 10 == 0:\n",
    "    #    if config['learning_rate'] > 0:\n",
    "    #        config['learning_rate'] = config['learning_rate'] * (1 - 1e-5)\n",
    "    pass # this function does nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mask(size, num_ones):\n",
    "    mask = np.zeros(size, dtype = np.float32)\n",
    "    index = list(range(size))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    for idx in index[:num_ones]:\n",
    "        mask[idx] = 1.0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_placeholders_tensors():\n",
    "    # get model's placeholders and tensors\n",
    "    x = tf.placeholder(tf.float32, name = 'x', shape = [None,\n",
    "                                                        config['num_channels'],\n",
    "                                                        config['x_height'],\n",
    "                                                        config['x_width']])\n",
    "    dropout_rate = tf.placeholder(tf.float32, name = 'dropout_rate', shape = [2])\n",
    "    is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
    "    triplets = tf.placeholder(tf.int32, name = 'triplets', shape = [None, 3])\n",
    "    same_threshold = tf.placeholder(tf.float32, name = 'same_threshold')\n",
    "    mask = tf.placeholder(tf.float32, name='mask')\n",
    "    \n",
    "\n",
    "    embeddings1, embeddings2 = build_model(x, dropout_rate, is_training, print_summary = True)\n",
    "\n",
    "    triplet_embedings1 = prepare_triplet_embeddings(triplets, embeddings1)\n",
    "    triplet_embedings2 = prepare_triplet_embeddings(triplets, embeddings2)\n",
    "    loss, loss2, ap_acc, an_acc, triplets_used  = triplet_loss_acc(triplet_embedings1, triplet_embedings2, mask, same_threshold)\n",
    "    optim = optimizer(loss)\n",
    "\n",
    "    placeholders_tensors = {'x': x,\n",
    "                            'dropout_rate': dropout_rate,\n",
    "                            'is_training': is_training,\n",
    "                            'triplets': triplets,\n",
    "                            'embeddings1': embeddings1,\n",
    "                            'embeddings2': embeddings2,\n",
    "                            'same_threshold': same_threshold,\n",
    "                            'mask': mask,\n",
    "                            'optimizer': optim,\n",
    "                            'loss': loss2,\n",
    "                            'ap_acc': ap_acc,\n",
    "                            'an_acc': an_acc,\n",
    "                            'triplets_used': triplets_used}\n",
    "    return placeholders_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_per_batch(sess, batch, placeholders_tensors, epoch, threshold):\n",
    "    # train_per_batch and get train loss\n",
    "    tmp_loss, tmp_ap_acc, tmp_an_acc, tmp_triplets_used = [], [], [], []\n",
    "    t_total = 0\n",
    "    bad_batch_counter = 0\n",
    "\n",
    "    cool_down_mask(epoch)\n",
    "    decrease_learning_rate(epoch)\n",
    "    for iteration in range(train_batches_count):\n",
    "        t_start = time.time()\n",
    "        batch_x = sess.run(batch)\n",
    "        batch_triplets = triplet_list_maker()\n",
    "        train_mask = build_mask(config['triplets_count'], config['masked_count'])\n",
    "        feed_dictionary = {placeholders_tensors['x']: batch_x,\n",
    "                           placeholders_tensors['dropout_rate']: config['dropout'],\n",
    "                           placeholders_tensors['is_training']: True,\n",
    "                           placeholders_tensors['triplets']: batch_triplets,\n",
    "                           placeholders_tensors['same_threshold']: threshold,\n",
    "                           placeholders_tensors['mask']: train_mask}\n",
    "\n",
    "        sess.run(placeholders_tensors['optimizer'], feed_dict = feed_dictionary)\n",
    "        train_loss = sess.run(placeholders_tensors['loss'], feed_dict = feed_dictionary)\n",
    "        train_ap_acc = sess.run(placeholders_tensors['ap_acc'], feed_dict = feed_dictionary)\n",
    "        train_an_acc = sess.run(placeholders_tensors['an_acc'], feed_dict = feed_dictionary)\n",
    "        train_triplets_used = sess.run(placeholders_tensors['triplets_used'], feed_dict = feed_dictionary)  \n",
    "\n",
    "        tmp_loss.append(train_loss)\n",
    "        tmp_ap_acc.append(train_ap_acc)\n",
    "        tmp_an_acc.append(train_an_acc)\n",
    "        tmp_triplets_used.append(train_triplets_used)\n",
    "\n",
    "        if int(train_triplets_used) != config ['triplets_count']:\n",
    "            bad_batch_counter += 1\n",
    "        t_total += (time.time() - t_start)\n",
    "\n",
    "        print(' '*200, end = '\\r')\n",
    "        print('epoch: %d, time: %f | train_loss: %f |' \\\n",
    "              ' ap_acc: %f | an_acc: %f | triplets_used: %d | bads: %d' %(epoch, t_total, train_loss,\n",
    "                                                               train_ap_acc, train_an_acc,\n",
    "                                                               int(train_triplets_used), bad_batch_counter),\n",
    "                                                               end = '\\r')\n",
    "    train_loss = np.mean(tmp_loss)\n",
    "    train_ap_acc = np.mean(tmp_ap_acc)\n",
    "    train_an_acc = np.mean(tmp_an_acc)\n",
    "    train_triplets_used = np.mean(tmp_triplets_used)\n",
    "    print(' '*200, end = '\\r')\n",
    "    print('epoch: %d, time: %f | train_loss: %f |' \\\n",
    "          ' ap_acc: %f | an_acc: %f | triplets_used: %d | bads: %d\\n' %(epoch, t_total, train_loss,\n",
    "                                                                       train_ap_acc, train_an_acc,\n",
    "                                                                       int(train_triplets_used), bad_batch_counter),\n",
    "                                                                       end = '\\r')\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_per_batch(sess, batch, placeholders_tensors, epoch, threshold):\n",
    "    # cross-validate per batch and get cv loss and accuracy\n",
    "    tmp_loss, tmp_ap_acc, tmp_an_acc, tmp_triplets_used = [], [], [], []\n",
    "    t_total = 0\n",
    "    bad_batch_counter = 0\n",
    "\n",
    "    for iteration in range(cv_batches_count):\n",
    "        t_start = time.time()\n",
    "        batch_x = sess.run(batch)\n",
    "        batch_triplets = triplet_list_maker()\n",
    "        test_mask = build_mask(config['triplets_count'], config['masked_count'])\n",
    "        cv_feed_dictionary = {placeholders_tensors['x']: batch_x,\n",
    "                              placeholders_tensors['dropout_rate']: [0.0, 0.0],\n",
    "                              placeholders_tensors['is_training']: False,\n",
    "                              placeholders_tensors['triplets']: batch_triplets,\n",
    "                              placeholders_tensors['same_threshold']: threshold,\n",
    "                              placeholders_tensors['mask']: test_mask}\n",
    "        \n",
    "        cv_loss = sess.run(placeholders_tensors['loss'], feed_dict = cv_feed_dictionary)\n",
    "        cv_ap_acc = sess.run(placeholders_tensors['ap_acc'], feed_dict = cv_feed_dictionary)\n",
    "        cv_an_acc = sess.run(placeholders_tensors['an_acc'], feed_dict = cv_feed_dictionary)\n",
    "        cv_triplets_used = sess.run(placeholders_tensors['triplets_used'], feed_dict = cv_feed_dictionary)\n",
    "\n",
    "        tmp_loss.append(cv_loss)\n",
    "        tmp_ap_acc.append(cv_ap_acc)\n",
    "        tmp_an_acc.append(cv_an_acc)\n",
    "        tmp_triplets_used.append(cv_triplets_used)\n",
    "\n",
    "        if int(cv_triplets_used) != config['triplets_count']:\n",
    "            bad_batch_counter += 1\n",
    "        t_total += (time.time() - t_start)\n",
    "\n",
    "        print(' '*200, end = '\\r')\n",
    "        print('time: %f | cv_loss: %f |' \\\n",
    "              ' cv_ap_acc: %f | cv_an_acc: %f | cv_triplets_used: %d | cv_bads: %d' %(t_total, cv_loss,\n",
    "                                                                                      cv_ap_acc, cv_an_acc,\n",
    "                                                                                      int(cv_triplets_used),\n",
    "                                                                                      bad_batch_counter), end = '\\r')\n",
    "    cv_loss = np.mean(tmp_loss)\n",
    "    cv_ap_acc = np.mean(tmp_ap_acc)\n",
    "    cv_an_acc = np.mean(tmp_an_acc)\n",
    "    cv_triplets_used = np.mean(tmp_triplets_used)\n",
    "    print(' '*200, end = '\\r')\n",
    "    print('time: %f | cv_loss: %f |' \\\n",
    "          ' cv_ap_acc: %f | cv_an_acc: %f | cV_triplets_used: %d | cV_bads: %d' %(t_total, cv_loss,\n",
    "                                                                                  cv_ap_acc, cv_an_acc,\n",
    "                                                                                  int(cv_triplets_used),\n",
    "                                                                                  bad_batch_counter))\n",
    "    return cv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, init_epoch, queue_size, num_threads, resume, threshold):\n",
    "    # train model\n",
    "    train_losses, cv_losses = [], []\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    placeholders_tensors = get_placeholders_tensors()\n",
    "    train_batch  = read_from_tfrecord([config['tfrecords_train']], queue_size, num_threads,\n",
    "                                      np.int(0.5 * queue_size))\n",
    "    cv_batch  = read_from_tfrecord([config['tfrecords_cv']], 50, 2, 10)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "\n",
    "        if resume:\n",
    "            print('loading weights....')\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, (config['model_path']))  # to load the best saved model so far replace\n",
    "                                                              # \"curr_model_path\" with \"model_path\"\n",
    "\n",
    "            # load loss and accuracy so that less accurate model\n",
    "            # won't be saved after resume\n",
    "            tmp = np.genfromtxt(config['log_path'], delimiter = ',', names = True)\n",
    "            train_losses = list(tmp['train_loss'])\n",
    "            cv_losses = list(tmp['cv_loss'])\n",
    "            del tmp\n",
    "        else:\n",
    "            print('initializing weights....')\n",
    "            init_op =tf.group(tf.local_variables_initializer(), tf.global_variables_initializer())\n",
    "            sess.run(init_op)\n",
    "\n",
    "        print('training....')\n",
    "        for epoch in range(init_epoch, epochs):\n",
    "            # training\n",
    "            train_loss = train_per_batch(sess,train_batch, \n",
    "                                         placeholders_tensors, epoch, threshold)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            #cross-validation\n",
    "            cv_loss = cv_per_batch(sess, cv_batch, \n",
    "                                   placeholders_tensors, epoch, threshold)\n",
    "\n",
    "            #save model\n",
    "            save_model_on_imporvemnet(config['model_path'], sess, cv_loss, cv_losses)\n",
    "            cv_losses.append(cv_loss)\n",
    "\n",
    "            # log results\n",
    "            log_loss(config['log_path'], epoch, train_loss, cv_loss,\n",
    "                     log_mode = ('a' if resume else 'w'))\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        return train_losses, cv_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(epochs = 1000, init_epoch = 8,\n",
    "            queue_size = 500, num_threads = 10,\n",
    "            resume = False, threshold = 0.84)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
